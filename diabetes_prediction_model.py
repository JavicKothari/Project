# -*- coding: utf-8 -*-
"""Diabetes Prediction Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qlX00X5rgCnS2E03K2oDzVpwNXPW2IrP

# **DIABETES PREDICTION MODEL**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import pickle
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn import svm
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix , ConfusionMatrixDisplay , accuracy_score , precision_score , recall_score , f1_score

data = pd.read_csv("diabetes.csv")
print(data.head())

data.shape

data.info()

data = data.rename(columns={'DiabetesPedigreeFunction':'DiabetesPedigree' })

data.isnull().sum()

plt.style.use('bmh')
ax = sns.countplot(x= data.Outcome , data= data)
ax.set_title('Suffering from diabetes or not')
plt.show()

np.sort(data.Age.unique())

plt.figure(figsize=(16,4))
plt.xticks(rotation = 90)
ax = sns.countplot(x= data.Age , hue= data.Outcome)
ax.set_title('Suffering from diabetes or not by Age')
plt.show()

np.sort(data.BloodPressure.unique())

plt.figure(figsize=(16,4))
plt.xticks(rotation = 90)
ax = sns.countplot(x= data.BloodPressure , hue= data.Outcome)
ax.set_title('Suffering from diabetes or not by bloodpressure')
plt.show()

np.sort(data.Insulin.unique())

X= data.drop(['Outcome'] , axis=1)
y= data['Outcome']
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.20)

from sklearn.svm import SVC
clf = svm.SVC().fit(X_train, y_train)
acc_train = clf.score(X_train, y_train)
y_pred = clf.predict(X_test)
acc_test = accuracy_score(y_pred , y_test)
print('accuracy_train: ',round(acc_train*100,2),'%')
print('accuracy_test: ', round(acc_test*100,2),'%')
print('precision_score: ', round(precision_score(y_test, y_pred),2))
print('recall_score: ', round(recall_score(y_test, y_pred),2))
print('f1_score: ', round(f1_score(y_test, y_pred),2))
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot()
plt.show()

#splitting dataset into 80:20 ratio
train_X,test_X,train_y,test_y=train_test_split(X,y,test_size=0.2)

train_X.shape,test_X.shape,train_y.shape,test_y.shape

from sklearn.metrics import confusion_matrix,accuracy_score,make_scorer
from sklearn.model_selection import cross_validate

def tn(y_true, y_pred): return confusion_matrix(y_true, y_pred)[0, 0]
def fp(y_true, y_pred): return confusion_matrix(y_true, y_pred)[0, 1]
def fn(y_true, y_pred): return confusion_matrix(y_true, y_pred)[1, 0]
def tp(y_true, y_pred): return confusion_matrix(y_true, y_pred)[1, 1]

#cross validation purpose
scoring = {'accuracy': make_scorer(accuracy_score),'prec': 'precision'}
scoring = {'tp': make_scorer(tp), 'tn': make_scorer(tn),
           'fp': make_scorer(fp), 'fn': make_scorer(fn)}

def display_result(result):
    print("TP: ",result['test_tp'])
    print("TN: ",result['test_tn'])
    print("FN: ",result['test_fn'])
    print("FP: ",result['test_fp'])

#logistic regression
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score

#random forest

from sklearn.ensemble import RandomForestClassifier as rc
rfc= rc(n_estimators=200)
rfc.fit(train_X,train_y)

#accuracy test for rfc

rfc_train=rfc.predict(train_X)
from sklearn import metrics as m
predictions = rfc.predict(X_test)
print("accuracy score= ", format(m.accuracy_score(y_test, predictions )))

# svm model

from sklearn.svm import SVC

svc_model = SVC()
svc_model.fit(X_train, y_train)
svc_pred = svc_model.predict(X_test)


print("Accuracy Score =", format(m.accuracy_score(y_test, svc_pred)))

#NAIES BAYES
from sklearn.naive_bayes import GaussianNB
nbModel = GaussianNB()
nbModel.fit(X_train, y_train)

nb_y_pred = nbModel.predict(X_test)
nbConfusion = m.confusion_matrix(y_test, nb_y_pred)
nbConfusion
ylabel = ["Actual [Non-Diab]","Actual [Diab]"]
xlabel = ["Pred [Non-Diab]","Pred [Diab]"]
#sns.set(font_scale = 1.5)
plt.figure(figsize=(15,6))
sns.heatmap(nbConfusion, annot=True, xticklabels = xlabel, yticklabels = ylabel, linecolor='white', linewidths=1)
print("Accuracy:", accuracy_score(y_test, y_pred))

print('Accuracy of Naive Bayes Classifier is: ', nbModel.score(X_test,y_test) * 100,'%')

from sklearn.neighbors import KNeighborsClassifier
def knn(X_train, y_train, X_test, y_test,n):
    n_range = range(1, n)
    results = []
    for n in n_range:
        knn = KNeighborsClassifier(n_neighbors=n)
        knn.fit(X_train, y_train)
        #Predict the response for test dataset
        predict_y = knn.predict(X_test)
        accuracy = m.accuracy_score(y_test, predict_y)
        #matrix = confusion_matrix(y_test,predict_y)
        #seaborn_matrix = sns.heatmap(matrix, annot = True, cmap="Blues",cbar=True)
        results.append(accuracy)
    return results

n= 300
output = knn(X_train,y_train,X_test,y_test,n)
n_range = range(1, n)
plt.plot(n_range, output)

#logistic regression has highest accuracy 92%
#model can be improve more if we take same count of labels
#in our model 35% is diabetic and 65% no diabetic patient

#model can be improve with fine tunning